{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \\[1\\]:\n",
    "\n",
    "    import selenium\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.support import expected_conditions as ec\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "    import time as t\n",
    "    from bs4 import *\n",
    "    from instascrape import Profile\n",
    "    import requests\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import base64\n",
    "    from datetime import datetime\n",
    "    from instagramy import InstagramUser\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    class FI_Data_Handling:\n",
    "        def str_num_conv(self,x):\n",
    "            total= 0\n",
    "            num_map = {'K':1000, 'M':1000000, 'B':1000000000}\n",
    "            if x.isdigit():\n",
    "                total= int(x)\n",
    "            else:\n",
    "                if len(x) > 1:\n",
    "                    total = float(x[:-1]) * num_map.get(x[-1].upper(), 1)\n",
    "            return int(total)\n",
    "\n",
    "\n",
    "    class Driver_Init:\n",
    "        def initialize_driver(self,driver=\"firefox\"):\n",
    "            if driver.lower()==\"firefox\":\n",
    "                options = FirefoxOptions()\n",
    "                options.add_argument(\"--headless\")\n",
    "                options.add_argument('--no-proxy-server')\n",
    "                options.add_argument('--disable-dev-shm-usage')\n",
    "                options.add_argument('--disable-gpu')\n",
    "                options.add_argument('log-level=3')\n",
    "                driver = webdriver.Firefox(options=options)\n",
    "                return driver\n",
    "            \n",
    "            else:\n",
    "                chrome_options = webdriver.ChromeOptions()\n",
    "                prefs = {\"profile.default_content_setting_values.notifications\" :2}\n",
    "                chrome_options.add_experimental_option(\"prefs\",prefs)\n",
    "                chrome_options.add_argument('--no-proxy-server')\n",
    "                chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "                chrome_options.add_argument('--disable-gpu')\n",
    "                chrome_options.add_argument('log-level=3')\n",
    "                chrome_options.add_argument('--headless')\n",
    "                driver = webdriver.Chrome(options=chrome_options)\n",
    "                return driver\n",
    "\n",
    "        def get_sessionID(self):\n",
    "            try:\n",
    "                link = 'https://www.instagram.com/accounts/login/'\n",
    "                login_url = 'https://www.instagram.com/accounts/login/ajax/'\n",
    "                pas=base64.b64decode(\"V2hpc3B5MDAx\").decode(\"utf-8\")\n",
    "                user=base64.b64decode(\"V2hpc3BlcnMzMzY2\").decode(\"utf-8\")\n",
    "                time = int(datetime.now().timestamp())\n",
    "\n",
    "                payload = {\n",
    "                    'username': user,\n",
    "                    'enc_password': f'#PWD_INSTAGRAM_BROWSER:0:{time}:{pas}',\n",
    "                    'queryParams': {},\n",
    "                    'optIntoOneTap': 'false'\n",
    "                }\n",
    "\n",
    "                with requests.Session() as s:\n",
    "                    r = s.get(link)\n",
    "                    csrf = re.findall(r\"csrf_token\\\":\\\"(.*?)\\\"\",r.text)[0]\n",
    "                    r = s.post(login_url,data=payload,headers={\n",
    "                        \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\",\n",
    "                        \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "                        \"Referer\": \"https://www.instagram.com/accounts/login/\",\n",
    "                        \"x-csrftoken\":csrf\n",
    "                    })\n",
    "\n",
    "                session_id=s.cookies.get_dict()[\"sessionid\"]\n",
    "                return session_id\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Unable to get Session ID!!\")\n",
    "            \n",
    "\n",
    "\n",
    "    class Face_Scraping:\n",
    "        def __init__(self,driver=\"firefox\"):   \n",
    "            self.driver=Driver_Init().initialize_driver(driver)\n",
    "            \n",
    "        def get_page_info(self,url=None,name=None):\n",
    "            \n",
    "            if url is None:\n",
    "                url=\"https://www.facebook.com/\"+name+\"/about/\"\n",
    "                \n",
    "                \n",
    "            if name is None:\n",
    "                if url[-1]==\"/\":\n",
    "                    name=re.findall(r'/[^/]+/([^/]+)/',url)[0]\n",
    "                else:\n",
    "                    name=re.findall(r'/[^/]+/([^/]+)',url)[0]\n",
    "                 \n",
    "                url=\"https://www.facebook.com/\"+name+\"/about/\"\n",
    "                \n",
    "            \n",
    "            self.driver.get(url)\n",
    "                \n",
    "            if \"Page not found\" in self.driver.title:\n",
    "                print(\"Sorry, This Page is Not Available!!\")\n",
    "                self.driver.close()\n",
    "                \n",
    "            else:\n",
    "                page_info={\"pg_likes\":0,\n",
    "                           \"pg_follows\":0,\n",
    "                           'pg_checked': 0,\n",
    "                           'pg_about': '.'\n",
    "                           }\n",
    "                \n",
    "                try:\n",
    "                    t.sleep(1)\n",
    "                    soup=BeautifulSoup(self.driver.page_source,\"html.parser\")\n",
    "\n",
    "                    all_info=soup.find_all(\"div\",{\"class\":\"dati1w0a ihqw7lf3 hv4rvrfc discj3wi d2edcug0\"+\n",
    "                                                   \" f9o22wc5 nzypyw8j ad2k81qe tr9rh885 rq0escxv l82x9zwi\"+\n",
    "                                                   \" uo3d90p7 pw54ja7n ue3kfks5 hybvsw6c\"})\n",
    "\n",
    "                    collected_info=all_info[0].find_all(\"div\",{\"class\":\"qzhwtbm6 knvmm38d\"})\n",
    "                    collected_info=[x.text for x in collected_info if x.text!=\"About\"]\n",
    "                    for q in collected_info:\n",
    "                        if \"like\" in q:\n",
    "                            page_info[\"pg_likes\"]=int(\"\".join(re.findall(r'\\d+',q)))\n",
    "\n",
    "                        elif \"follow\" in q:\n",
    "                            page_info[\"pg_follows\"]=int(\"\".join(re.findall(r'\\d+',q)))\n",
    "\n",
    "                        elif \"checked\" in q:\n",
    "                            page_info[\"pg_checked\"]=int(\"\".join(re.findall(r'\\d+',q)))\n",
    "\n",
    "                        elif 'About' in q:\n",
    "                            page_info[\"pg_about\"]=q[5:]\n",
    "\n",
    "                    self.driver.close()\n",
    "                    return page_info\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    self.driver.close()\n",
    "                    \n",
    "                    \n",
    "        def get_page_posts(self,url=None,name=None):\n",
    "            \n",
    "            if url is None:\n",
    "                url=\"https://www.facebook.com/pg/\"+name+\"/posts/\"\n",
    "                \n",
    "                \n",
    "            if name is None:\n",
    "                if url[-1]==\"/\":\n",
    "                    name=re.findall(r'/[^/]+/([^/]+)/',url)[0]\n",
    "                else:\n",
    "                    name=re.findall(r'/[^/]+/([^/]+)',url)[0]\n",
    "                 \n",
    "                url=\"https://www.facebook.com/pg/\"+name+\"/posts/\"\n",
    "                \n",
    "            \n",
    "            self.driver.get(url)\n",
    "            \n",
    "            if \"Page not found\" in self.driver.title:\n",
    "                print(\"Sorry, This Page is Not Available!!\")\n",
    "                self.driver.close()\n",
    "            \n",
    "            else:\n",
    "                t.sleep(1)\n",
    "                comments=[]\n",
    "                reactions=[]\n",
    "                shares=[]\n",
    "                contents=[]\n",
    "                timestamps=[]\n",
    "                soup=BeautifulSoup(self.driver.page_source,\"html.parser\")\n",
    "                all_posts=soup.find_all(\"div\",{\"class\":\"_427x\"})\n",
    "\n",
    "                for post in all_posts:\n",
    "                    try:\n",
    "                        comment=post.find(\"a\",{\"class\":\"_3hg- _42ft\"})\n",
    "                        if comment is None:\n",
    "                            comment=BeautifulSoup(\"<p>0 Comments</p>\",\"lxml\")\n",
    "                    except Exception:\n",
    "                        comment=\"Not Found\"\n",
    "                        print(comment)  \n",
    "                        \n",
    "                    try:\n",
    "                        timestamp=post.find(\"abbr\",{\"class\":re.compile(r\"_5ptz\")})\n",
    "                    except Exception:\n",
    "                        timestamp=\"Not Found\"\n",
    "                        print(timestamp)\n",
    "\n",
    "                    try:\n",
    "                        reaction={}\n",
    "                        tot=post.find(\"span\",{\"class\":\"_81hb\"})\n",
    "                        if tot is None:\n",
    "                            tot=BeautifulSoup(\"<p>0</p>\",\"lxml\")\n",
    "\n",
    "                        like=post.find(\"a\",{\"class\":\"_1n9l\",\"aria-label\":re.compile(r'Like')})\n",
    "                        if like is None:\n",
    "                            like=BeautifulSoup(\"<a aria-label='0 Like'>0</a>\",\"html.parser\").find('a')\n",
    "\n",
    "                        love=post.find(\"a\",{\"class\":\"_1n9l\",\"aria-label\":re.compile(r'Love')})\n",
    "                        if love is None:\n",
    "                            love=BeautifulSoup(\"<a aria-label='0 Love'>0</a>\",\"html.parser\").find('a')\n",
    "\n",
    "                        sad=post.find(\"a\",{\"class\":\"_1n9l\",\"aria-label\":re.compile(r'Sad')})\n",
    "                        if sad is None:\n",
    "                            sad=BeautifulSoup(\"<a aria-label='0 Sad'>0</a>\",\"html.parser\").find('a')\n",
    "\n",
    "                        angry=post.find(\"a\",{\"class\":\"_1n9l\",\"aria-label\":re.compile(r'Angry')})\n",
    "                        if angry is None:\n",
    "                            angry=BeautifulSoup(\"<a aria-label='0 Angry'>0</a>\",\"html.parser\").find('a')\n",
    "\n",
    "                        haha=post.find(\"a\",{\"class\":\"_1n9l\",\"aria-label\":re.compile(r'Haha')})\n",
    "                        if haha is None:\n",
    "                            haha=BeautifulSoup(\"<a aria-label='0 Haha'>0</a>\",\"html.parser\").find('a')\n",
    "\n",
    "                        fi_h=FI_Data_Handling()\n",
    "                        reaction[\"tot\"]=fi_h.str_num_conv(tot.text)\n",
    "                        reaction[\"like\"]=fi_h.str_num_conv(like[\"aria-label\"].split()[0])\n",
    "                        reaction[\"love\"]=fi_h.str_num_conv(love[\"aria-label\"].split()[0])\n",
    "                        reaction[\"sad\"]=fi_h.str_num_conv(sad[\"aria-label\"].split()[0])\n",
    "                        reaction[\"haha\"]=fi_h.str_num_conv(haha[\"aria-label\"].split()[0])\n",
    "                        reaction[\"angry\"]=fi_h.str_num_conv(angry[\"aria-label\"].split()[0])\n",
    "                        remaining_reacts=fi_h.str_num_conv(tot.text)-(fi_h.str_num_conv(like[\"aria-label\"].split()[0])+\n",
    "                                                                      fi_h.str_num_conv(love[\"aria-label\"].split()[0])+\n",
    "                                                                      fi_h.str_num_conv(sad[\"aria-label\"].split()[0])+\n",
    "                                                                      fi_h.str_num_conv(angry[\"aria-label\"].split()[0])+\n",
    "                                                                      fi_h.str_num_conv(haha[\"aria-label\"].split()[0])\n",
    "                                                                          )\n",
    "\n",
    "                        reaction[\"r_reacts\"]=abs(remaining_reacts)\n",
    "\n",
    "\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                    try:\n",
    "                        share=post.find(\"span\",{\"class\":\"_355t _4vn2\"})\n",
    "                        if share is None:\n",
    "                            share=BeautifulSoup(\"<p>0 Shares</p>\",\"lxml\")\n",
    "                    except Exception:\n",
    "                        share=\"Not Found\"\n",
    "                        print(share)\n",
    "\n",
    "                    try:\n",
    "                        content=post.find(\"div\",{\"class\":\"_5pbx userContent _3576\"})\n",
    "                        if content is None:\n",
    "                            content=post.find(\"div\",{\"class\":\"_5_jv _58jw\"})\n",
    "                            if content is None:\n",
    "                                content=BeautifulSoup(\"<p>No Content</p>\",\"lxml\")\n",
    "                    except Exception:\n",
    "                        content=\"Not Found\"\n",
    "                        print(content)\n",
    "\n",
    "                    timestamps.append(timestamp[\"data-utime\"])\n",
    "                    contents.append(content.text)\n",
    "                    comments.append(fi_h.str_num_conv(comment.text.split()[0]))\n",
    "                    shares.append(fi_h.str_num_conv(share.text.split()[0]))\n",
    "                    reactions.append(reaction)\n",
    "                df=pd.DataFrame({\n",
    "                                 \"timestamp\":timestamps,\n",
    "                                 \"upload_date\":[datetime.fromtimestamp(int(x)) for x in timestamps],\n",
    "                                 \"Content\":contents,\n",
    "                                 \"Num_Comments\":comments,\n",
    "                                 \"Num_Shares\":shares,\n",
    "                                 \"Total_Reacts\":[x[\"tot\"] for x in reactions],\n",
    "                                 \"Likes\":[x[\"like\"] for x in reactions],\n",
    "                                 \"Sad\":[x[\"sad\"] for x in reactions],\n",
    "                                 \"Angry\":[x[\"angry\"] for x in reactions],\n",
    "                                 \"Love\":[x[\"love\"] for x in reactions],\n",
    "                                 \"Haha\":[x[\"haha\"] for x in reactions],\n",
    "                                 \"Remaining_reacts\":[x[\"r_reacts\"] for x in reactions],\n",
    "                                })\n",
    "\n",
    "                df.drop_duplicates(subset=\"Content\",inplace=True,keep=\"first\")\n",
    "\n",
    "                self.driver.close()\n",
    "\n",
    "                return df\n",
    "            \n",
    "    class Insta_Scraping:\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.sess_id=Driver_Init().get_sessionID()\n",
    "            \n",
    "        def get_page_posts_info(self,url):\n",
    "            try:\n",
    "                session_id=self.sess_id\n",
    "                profile_page = Profile(url)\n",
    "                headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.74 Safari/537.36 Edg/79.0.309.43\",\n",
    "                \"cookie\":f'sessionid={session_id};'\n",
    "                }\n",
    "                profile_page.scrape(headers=headers)\n",
    "                page_info=pd.DataFrame(profile_page.to_dict(),index=[x for x in range(1)])\n",
    "                recent_posts =profile_page.get_recent_posts()\n",
    "                posts_data = [post.to_dict() for post in recent_posts]\n",
    "                recent_posts_df = pd.DataFrame(posts_data)\n",
    "\n",
    "                return page_info,recent_posts_df\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Unable To Get Insta Profile Info!!!\")\n",
    "                \n",
    "                \n",
    "        def insta_2_scraper(self,url):\n",
    "            try:\n",
    "                session_id=self.sess_id\n",
    "                user = InstagramUser(url, sessionid=session_id)\n",
    "                page_info={}\n",
    "                page_info[\"number_of_followers\"]=user.number_of_followers\n",
    "                page_info[\"number_of_followings\"]=user.number_of_followings\n",
    "                page_info[\"number_of_posts\"]=user.number_of_posts\n",
    "                page_info[\"profile_picture_url\"]=user.profile_picture_url\n",
    "                page_info[\"is_verified\"]=user.is_verified\n",
    "                page_info[\"other_info\"]=user.other_info\n",
    "                page_info[\"username\"]=user.username\n",
    "                page_info[\"fullname\"]=user.fullname\n",
    "                page_info[\"is_private\"]=user.is_private\n",
    "                page_info[\"is_joined_recently\"]=user.is_joined_recently\n",
    "                page_info[\"biography\"]=user.biography\n",
    "                page_info[\"website\"]=user.website\n",
    "                page_info_df=pd.DataFrame(page_info,index=[x for x in range(1)])\n",
    "\n",
    "                likes=[]\n",
    "                comments=[]\n",
    "                upload_date=[]\n",
    "                caption=[]\n",
    "                timestamp=[]\n",
    "                display_url=[]\n",
    "                is_video=[]\n",
    "                short_code=[]\n",
    "                post_url=[]\n",
    "                location=[]\n",
    "                for post in user.posts:\n",
    "                    likes.append(post.likes)\n",
    "                    comments.append(post.comments)\n",
    "                    upload_date.append(post.taken_at_timestamp)\n",
    "                    caption.append(post.caption)\n",
    "                    timestamp.append(post.timestamp)\n",
    "                    display_url.append(post.display_url)\n",
    "                    is_video.append(post.is_video)\n",
    "                    short_code.append(post.shortcode)\n",
    "                    post_url.append(post.post_url)\n",
    "                    location.append(post.location)\n",
    "\n",
    "                posts_info_df=pd.DataFrame({\n",
    "                                            \"Likes\":likes,\n",
    "                                            \"Num_Comments\":comments,\n",
    "                                            \"upload_date\":upload_date,\n",
    "                                            \"caption\":caption,\n",
    "                                            \"timestamp\":timestamp,\n",
    "                                            \"display_url\":display_url,\n",
    "                                            \"is_video\":is_video,\n",
    "                                            \"short_code\":short_code,\n",
    "                                            \"post_url\":post_url,\n",
    "                                            \"location\":location\n",
    "                                            })\n",
    "\n",
    "                return page_info_df,posts_info_df\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        \n",
    "\n",
    "In \\[3\\]:\n",
    "\n",
    "    fi=Face_Scraping()\n",
    "    df=fi.get_page_posts(url=\"https://www.facebook.com/google\")\n",
    "\n",
    "In \\[4\\]:\n",
    "\n",
    "    fi=Face_Scraping()\n",
    "    df1=fi.get_page_info(url=\"https://www.facebook.com/google\")\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    ins=Insta_Scraping()\n",
    "    cur_info,cur_posts=ins.insta_2_scraper(\"google\")\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    df.head()\n",
    "\n",
    "Out\\[6\\]:\n",
    "\n",
    "|     | timestamp  | upload_date         | Content                                           | Num_Comments | Num_Shares | Total_Reacts | Likes | Sad | Angry | Love | Haha | Remaining_reacts |\n",
    "|-----|------------|---------------------|---------------------------------------------------|--------------|------------|--------------|-------|-----|-------|------|------|------------------|\n",
    "| 0   | 1652299670 | 2022-05-11 22:07:50 | At #GoogleIO today, Sundar Pichai talked about... | 163          | 38         | 611          | 553   | 0   | 0     | 48   | 0    | 10               |\n",
    "| 1   | 1653001245 | 2022-05-20 01:00:45 | Engineering lead Gordon Kuo talks about landin... | 7            | 3          | 110          | 100   | 0   | 0     | 8    | 0    | 2                |\n",
    "| 2   | 1652982521 | 2022-05-19 19:48:41 | On Global Accessibility Awareness Day (#GAAD),... | 23           | 11         | 204          | 168   | 0   | 0     | 20   | 0    | 16               |\n",
    "| 3   | 1652975099 | 2022-05-19 17:44:59 | Today's #GoogleDoodle celebrates Asian Pacific... | 11           | 36         | 199          | 159   | 0   | 0     | 27   | 0    | 13               |\n",
    "| 4   | 1652904126 | 2022-05-18 22:02:06 | Our new Bay View campus features a first-of-it... | 75           | 54         | 739          | 640   | 0   | 0     | 76   | 0    | 23               |\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    pd.DataFrame(df1,index=[x for x in range(1)])\n",
    "\n",
    "Out\\[7\\]:\n",
    "\n",
    "|     | pg_likes | pg_follows | pg_checked | pg_about                                          |\n",
    "|-----|----------|------------|------------|---------------------------------------------------|\n",
    "| 0   | 28180257 | 33029117   | 0          | Organizing the world's information and making ... |\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    cur_info\n",
    "\n",
    "Out\\[8\\]:\n",
    "\n",
    "|     | number_of_followers | number_of_followings | number_of_posts | profile_picture_url                               | is_verified | other_info | username | fullname | is_private | is_joined_recently | biography                                 | website                   |\n",
    "|-----|---------------------|----------------------|-----------------|---------------------------------------------------|-------------|------------|----------|----------|------------|--------------------|-------------------------------------------|---------------------------|\n",
    "| 0   | 13031372            | 33                   | 1798            | https://instagram.fcai2-1.fna.fbcdn.net/v/t51.... | True        | NaN        | google   | Google   | False      | False              | Google unfiltered—sometimes with filters. | https://linkin.bio/google |\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    cur_posts.head()\n",
    "\n",
    "Out\\[9\\]:\n",
    "\n",
    "|     | Likes | Num_Comments | upload_date         | caption                                           | timestamp  | display_url                                       | is_video | short_code  | post_url                                 | location                                          |\n",
    "|-----|-------|--------------|---------------------|---------------------------------------------------|------------|---------------------------------------------------|----------|-------------|------------------------------------------|---------------------------------------------------|\n",
    "| 0   | 7750  | 86           | 2022-05-19 21:29:55 | An aerial view of the Google Bay View Campus, ... | 1652988595 | https://instagram.fcai2-1.fna.fbcdn.net/v/t51.... | False    | CdwFxosOdvb | https://www.instagram.com/p/CdwFxosOdvb/ | None                                              |\n",
    "| 1   | 6066  | 101          | 2022-05-19 17:56:34 | Google logo artwork with Stacey Milbern on the... | 1652975794 | https://instagram.fcai2-1.fna.fbcdn.net/v/t51.... | False    | CdvtXC7O3Xj | https://www.instagram.com/p/CdvtXC7O3Xj/ | None                                              |\n",
    "| 2   | 4363  | 87           | 2022-05-19 00:42:50 | None                                              | 1652913770 | https://instagram.fcai2-1.fna.fbcdn.net/v/t51.... | True     | Cdt3Bi-DzAK | https://www.instagram.com/p/Cdt3Bi-DzAK/ | None                                              |\n",
    "| 3   | 38132 | 309          | 2022-05-17 23:12:24 | A building at the Google Bay View campus, phot... | 1652821944 | https://instagram.fcai2-2.fna.fbcdn.net/v/t51.... | False    | CdrH6kiudWa | https://www.instagram.com/p/CdrH6kiudWa/ | None                                              |\n",
    "| 4   | 21376 | 309          | 2022-05-12 04:46:36 | Image of a colorful sculpture that spells out ... | 1652323596 | https://instagram.fcai2-1.fna.fbcdn.net/v/t51.... | False    | CdcRZE4LWZX | https://www.instagram.com/p/CdcRZE4LWZX/ | {'id': '216796294', 'has_public_page': True, '... |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee41ebc",
   "metadata": {},
   "source": [
    "In \\[1\\]:\n",
    "\n",
    "    import selenium\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.support import expected_conditions as ec\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "    import time as t\n",
    "    from bs4 import *\n",
    "    from instascrape import Profile\n",
    "    import requests\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import base64\n",
    "    from datetime import datetime\n",
    "    from instagramy import InstagramUser\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    class FI_Data_Handling:\n",
    "        def str_num_conv(self,x):\n",
    "            total= 0\n",
    "            num_map = {'K':1000, 'M':1000000, 'B':1000000000}\n",
    "            if x.isdigit():\n",
    "                total= int(x)\n",
    "            else:\n",
    "                if len(x) > 1:\n",
    "                    total = float(x[:-1]) * num_map.get(x[-1].upper(), 1)\n",
    "            return int(total)\n",
    "\n",
    "\n",
    "    class Driver_Init:\n",
    "        def initialize_driver(self,driver=\"firefox\"):\n",
    "            if driver.lower()==\"firefox\":\n",
    "                options = FirefoxOptions()\n",
    "                options.add_argument(\"--headless\")\n",
    "                options.add_argument('--no-proxy-server')\n",
    "                options.add_argument('--disable-dev-shm-usage')\n",
    "                options.add_argument('--disable-gpu')\n",
    "                options.add_argument('log-level=3')\n",
    "                driver = webdriver.Firefox(options=options)\n",
    "                return driver\n",
    "            \n",
    "            else:\n",
    "                chrome_options = webdriver.ChromeOptions()\n",
    "                prefs = {\"profile.default_content_setting_values.notifications\" :2}\n",
    "                chrome_options.add_experimental_option(\"prefs\",prefs)\n",
    "                chrome_options.add_argument('--no-proxy-server')\n",
    "                chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "                chrome_options.add_argument('--disable-gpu')\n",
    "                chrome_options.add_argument('log-level=3')\n",
    "                chrome_options.add_argument('--headless')\n",
    "                driver = webdriver.Chrome(options=chrome_options)\n",
    "                return driver\n",
    "\n",
    "        def get_sessionID(self):\n",
    "            try:\n",
    "                link = 'https://www.instagram.com/accounts/login/'\n",
    "                login_url = 'https://www.instagram.com/accounts/login/ajax/'\n",
    "                pas=base64.b64decode(\"V2hpc3B5MDAx\").decode(\"utf-8\")\n",
    "                user=base64.b64decode(\"V2hpc3BlcnMzMzY2\").decode(\"utf-8\")\n",
    "                time = int(datetime.now().timestamp())\n",
    "\n",
    "                payload = {\n",
    "                    'username': user,\n",
    "                    'enc_password': f'#PWD_INSTAGRAM_BROWSER:0:{time}:{pas}',\n",
    "                    'queryParams': {},\n",
    "                    'optIntoOneTap': 'false'\n",
    "                }\n",
    "\n",
    "                with requests.Session() as s:\n",
    "                    r = s.get(link)\n",
    "                    csrf = re.findall(r\"csrf_token\\\":\\\"(.*?)\\\"\",r.text)[0]\n",
    "                    r = s.post(login_url,data=payload,headers={\n",
    "                        \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\",\n",
    "                        \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "                        \"Referer\": \"https://www.instagram.com/accounts/login/\",\n",
    "                        \"x-csrftoken\":csrf\n",
    "                    })\n",
    "\n",
    "                session_id=s.cookies.get_dict()[\"sessionid\"]\n",
    "                return session_id\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Unable to get Session ID!!\")\n",
    "            \n",
    "\n",
    "\n",
    "    class Face_Scraping:\n",
    "        def __init__(self,driver=\"firefox\"):   \n",
    "            self.driver=Driver_Init().initialize_driver(driver)\n",
    "            \n",
    "        def get_page_info(self,url=None,name=None):\n",
    "            \n",
    "            if url is None:\n",
    "                url=\"https://www.facebook.com/\"+name+\"/about/\"\n",
    "                \n",
    "                \n",
    "            if name is None:\n",
    "                if url[-1]==\"/\":\n",
    "                    name=re.findall(r'/[^/]+/([^/]+)/',url)[0]\n",
    "                else:\n",
    "                    name=re.findall(r'/[^/]+/([^/]+)',url)[0]\n",
    "                 \n",
    "                url=\"https://www.facebook.com/\"+name+\"/about/\"\n",
    "                \n",
    "            \n",
    "            self.driver.get(url)\n",
    "                \n",
    "            if \"Page not found\" in self.driver.title:\n",
    "                print(\"Sorry, This Page is Not Available!!\")\n",
    "                self.driver.close()\n",
    "                \n",
    "            else:\n",
    "                page_info={\"pg_likes\":0,\n",
    "                           \"pg_follows\":0,\n",
    "                           'pg_checked': 0,\n",
    "                           'pg_about': '.'\n",
    "                           }\n",
    "                \n",
    "                try:\n",
    "                    t.sleep(1)\n",
    "                    soup=BeautifulSoup(self.driver.page_source,\"html.parser\")\n",
    "\n",
    "                    all_info=soup.find_all(\"div\",{\"class\":\"dati1w0a ihqw7lf3 hv4rvrfc discj3wi d2edcug0\"+\n",
    "                                                   \" f9o22wc5 nzypyw8j ad2k81qe tr9rh885 rq0escxv l82x9zwi\"+\n",
    "                                                   \" uo3d90p7 pw54ja7n ue3kfks5 hybvsw6c\"})\n",
    "\n",
    "                    collected_info=all_info[0].find_all(\"div\",{\"class\":\"qzhwtbm6 knvmm38d\"})\n",
    "                    collected_info=[x.text for x in collected_info if x.text!=\"About\"]\n",
    "                    for q in collected_info:\n",
    "                        if \"like\" in q:\n",
    "                            page_info[\"pg_likes\"]=int(\"\".join(re.findall(r'\\d+',q)))\n",
    "\n",
    "                        elif \"follow\" in q:\n",
    "                            page_info[\"pg_follows\"]=int(\"\".join(re.findall(r'\\d+',q)))\n",
    "\n",
    "                        elif \"checked\" in q:\n",
    "                            page_info[\"pg_checked\"]=int(\"\".join(re.findall(r'\\d+',q)))\n",
    "\n",
    "                        elif 'About' in q:\n",
    "                            page_info[\"pg_about\"]=q[5:]\n",
    "\n",
    "                    self.driver.close()\n",
    "                    return page_info\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    self.driver.close()\n",
    "                    \n",
    "                    \n",
    "        def get_page_posts(self,url=None,name=None):\n",
    "            \n",
    "            if url is None:\n",
    "                url=\"https://www.facebook.com/pg/\"+name+\"/posts/\"\n",
    "                \n",
    "                \n",
    "            if name is None:\n",
    "                if url[-1]==\"/\":\n",
    "                    name=re.findall(r'/[^/]+/([^/]+)/',url)[0]\n",
    "                else:\n",
    "                    name=re.findall(r'/[^/]+/([^/]+)',url)[0]\n",
    "                 \n",
    "                url=\"https://www.facebook.com/pg/\"+name+\"/posts/\"\n",
    "                \n",
    "            \n",
    "            self.driver.get(url)\n",
    "            \n",
    "            if \"Page not found\" in self.driver.title:\n",
    "                print(\"Sorry, This Page is Not Available!!\")\n",
    "                self.driver.close()\n",
    "            \n",
    "            else:\n",
    "                t.sleep(1)\n",
    "                comments=[]\n",
    "                reactions=[]\n",
    "                shares=[]\n",
    "                contents=[]\n",
    "                timestamps=[]\n",
    "                soup=BeautifulSoup(self.driver.page_source,\"html.parser\")\n",
    "                all_posts=soup.find_all(\"div\",{\"class\":\"_427x\"})\n",
    "\n",
    "                for post in all_posts:\n",
    "                    try:\n",
    "                        comment=post.find(\"a\",{\"class\":\"_3hg- _42ft\"})\n",
    "                        if comment is None:\n",
    "                            comment=BeautifulSoup(\"<p>0 Comments</p>\",\"lxml\")\n",
    "                    except Exception:\n",
    "                        comment=\"Not Found\"\n",
    "                        print(comment)  \n",
    "                        \n",
    "                    try:\n",
    "                        timestamp=post.find(\"abbr\",{\"class\":re.compile(r\"_5ptz\")})\n",
    "                    except Exception:\n",
    "                        timestamp=\"Not Found\"\n",
    "                        print(timestamp)\n",
    "\n",
    "                    try:\n",
    "                        reaction={}\n",
    "                        tot=post.find(\"span\",{\"class\":\"_81hb\"})\n",
    "                        if tot is None:\n",
    "                            tot=BeautifulSoup(\"<p>0</p>\",\"lxml\")\n",
    "\n",
    "                        like=post.find(\"a\",{\"class\":\"_1n9l\",\"aria-label\":re.compile(r'Like')})\n",
    "                        if like is None:\n",
    "                            like=BeautifulSoup(\"<a aria-label='0 Like'>0</a>\",\"html.parser\").find('a')\n",
    "\n",
    "                        love=post.find(\"a\",{\"class\":\"_1n9l\",\"aria-label\":re.compile(r'Love')})\n",
    "                        if love is None:\n",
    "                            love=BeautifulSoup(\"<a aria-label='0 Love'>0</a>\",\"html.parser\").find('a')\n",
    "\n",
    "                        sad=post.find(\"a\",{\"class\":\"_1n9l\",\"aria-label\":re.compile(r'Sad')})\n",
    "                        if sad is None:\n",
    "                            sad=BeautifulSoup(\"<a aria-label='0 Sad'>0</a>\",\"html.parser\").find('a')\n",
    "\n",
    "                        angry=post.find(\"a\",{\"class\":\"_1n9l\",\"aria-label\":re.compile(r'Angry')})\n",
    "                        if angry is None:\n",
    "                            angry=BeautifulSoup(\"<a aria-label='0 Angry'>0</a>\",\"html.parser\").find('a')\n",
    "\n",
    "                        haha=post.find(\"a\",{\"class\":\"_1n9l\",\"aria-label\":re.compile(r'Haha')})\n",
    "                        if haha is None:\n",
    "                            haha=BeautifulSoup(\"<a aria-label='0 Haha'>0</a>\",\"html.parser\").find('a')\n",
    "\n",
    "                        fi_h=FI_Data_Handling()\n",
    "                        reaction[\"tot\"]=fi_h.str_num_conv(tot.text)\n",
    "                        reaction[\"like\"]=fi_h.str_num_conv(like[\"aria-label\"].split()[0])\n",
    "                        reaction[\"love\"]=fi_h.str_num_conv(love[\"aria-label\"].split()[0])\n",
    "                        reaction[\"sad\"]=fi_h.str_num_conv(sad[\"aria-label\"].split()[0])\n",
    "                        reaction[\"haha\"]=fi_h.str_num_conv(haha[\"aria-label\"].split()[0])\n",
    "                        reaction[\"angry\"]=fi_h.str_num_conv(angry[\"aria-label\"].split()[0])\n",
    "                        remaining_reacts=fi_h.str_num_conv(tot.text)-(fi_h.str_num_conv(like[\"aria-label\"].split()[0])+\n",
    "                                                                      fi_h.str_num_conv(love[\"aria-label\"].split()[0])+\n",
    "                                                                      fi_h.str_num_conv(sad[\"aria-label\"].split()[0])+\n",
    "                                                                      fi_h.str_num_conv(angry[\"aria-label\"].split()[0])+\n",
    "                                                                      fi_h.str_num_conv(haha[\"aria-label\"].split()[0])\n",
    "                                                                          )\n",
    "\n",
    "                        reaction[\"r_reacts\"]=abs(remaining_reacts)\n",
    "\n",
    "\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                    try:\n",
    "                        share=post.find(\"span\",{\"class\":\"_355t _4vn2\"})\n",
    "                        if share is None:\n",
    "                            share=BeautifulSoup(\"<p>0 Shares</p>\",\"lxml\")\n",
    "                    except Exception:\n",
    "                        share=\"Not Found\"\n",
    "                        print(share)\n",
    "\n",
    "                    try:\n",
    "                        content=post.find(\"div\",{\"class\":\"_5pbx userContent _3576\"})\n",
    "                        if content is None:\n",
    "                            content=post.find(\"div\",{\"class\":\"_5_jv _58jw\"})\n",
    "                            if content is None:\n",
    "                                content=BeautifulSoup(\"<p>No Content</p>\",\"lxml\")\n",
    "                    except Exception:\n",
    "                        content=\"Not Found\"\n",
    "                        print(content)\n",
    "\n",
    "                    timestamps.append(timestamp[\"data-utime\"])\n",
    "                    contents.append(content.text)\n",
    "                    comments.append(fi_h.str_num_conv(comment.text.split()[0]))\n",
    "                    shares.append(fi_h.str_num_conv(share.text.split()[0]))\n",
    "                    reactions.append(reaction)\n",
    "                df=pd.DataFrame({\n",
    "                                 \"timestamp\":timestamps,\n",
    "                                 \"upload_date\":[datetime.fromtimestamp(int(x)) for x in timestamps],\n",
    "                                 \"Content\":contents,\n",
    "                                 \"Num_Comments\":comments,\n",
    "                                 \"Num_Shares\":shares,\n",
    "                                 \"Total_Reacts\":[x[\"tot\"] for x in reactions],\n",
    "                                 \"Likes\":[x[\"like\"] for x in reactions],\n",
    "                                 \"Sad\":[x[\"sad\"] for x in reactions],\n",
    "                                 \"Angry\":[x[\"angry\"] for x in reactions],\n",
    "                                 \"Love\":[x[\"love\"] for x in reactions],\n",
    "                                 \"Haha\":[x[\"haha\"] for x in reactions],\n",
    "                                 \"Remaining_reacts\":[x[\"r_reacts\"] for x in reactions],\n",
    "                                })\n",
    "\n",
    "                df.drop_duplicates(subset=\"Content\",inplace=True,keep=\"first\")\n",
    "\n",
    "                self.driver.close()\n",
    "\n",
    "                return df\n",
    "            \n",
    "    class Insta_Scraping:\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.sess_id=Driver_Init().get_sessionID()\n",
    "            \n",
    "        def get_page_posts_info(self,url):\n",
    "            try:\n",
    "                session_id=self.sess_id\n",
    "                profile_page = Profile(url)\n",
    "                headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.74 Safari/537.36 Edg/79.0.309.43\",\n",
    "                \"cookie\":f'sessionid={session_id};'\n",
    "                }\n",
    "                profile_page.scrape(headers=headers)\n",
    "                page_info=pd.DataFrame(profile_page.to_dict(),index=[x for x in range(1)])\n",
    "                recent_posts =profile_page.get_recent_posts()\n",
    "                posts_data = [post.to_dict() for post in recent_posts]\n",
    "                recent_posts_df = pd.DataFrame(posts_data)\n",
    "\n",
    "                return page_info,recent_posts_df\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Unable To Get Insta Profile Info!!!\")\n",
    "                \n",
    "                \n",
    "        def insta_2_scraper(self,url):\n",
    "            try:\n",
    "                session_id=self.sess_id\n",
    "                user = InstagramUser(url, sessionid=session_id)\n",
    "                page_info={}\n",
    "                page_info[\"number_of_followers\"]=user.number_of_followers\n",
    "                page_info[\"number_of_followings\"]=user.number_of_followings\n",
    "                page_info[\"number_of_posts\"]=user.number_of_posts\n",
    "                page_info[\"profile_picture_url\"]=user.profile_picture_url\n",
    "                page_info[\"is_verified\"]=user.is_verified\n",
    "                page_info[\"other_info\"]=user.other_info\n",
    "                page_info[\"username\"]=user.username\n",
    "                page_info[\"fullname\"]=user.fullname\n",
    "                page_info[\"is_private\"]=user.is_private\n",
    "                page_info[\"is_joined_recently\"]=user.is_joined_recently\n",
    "                page_info[\"biography\"]=user.biography\n",
    "                page_info[\"website\"]=user.website\n",
    "                page_info_df=pd.DataFrame(page_info,index=[x for x in range(1)])\n",
    "\n",
    "                likes=[]\n",
    "                comments=[]\n",
    "                upload_date=[]\n",
    "                caption=[]\n",
    "                timestamp=[]\n",
    "                display_url=[]\n",
    "                is_video=[]\n",
    "                short_code=[]\n",
    "                post_url=[]\n",
    "                location=[]\n",
    "                for post in user.posts:\n",
    "                    likes.append(post.likes)\n",
    "                    comments.append(post.comments)\n",
    "                    upload_date.append(post.taken_at_timestamp)\n",
    "                    caption.append(post.caption)\n",
    "                    timestamp.append(post.timestamp)\n",
    "                    display_url.append(post.display_url)\n",
    "                    is_video.append(post.is_video)\n",
    "                    short_code.append(post.shortcode)\n",
    "                    post_url.append(post.post_url)\n",
    "                    location.append(post.location)\n",
    "\n",
    "                posts_info_df=pd.DataFrame({\n",
    "                                            \"Likes\":likes,\n",
    "                                            \"Num_Comments\":comments,\n",
    "                                            \"upload_date\":upload_date,\n",
    "                                            \"caption\":caption,\n",
    "                                            \"timestamp\":timestamp,\n",
    "                                            \"display_url\":display_url,\n",
    "                                            \"is_video\":is_video,\n",
    "                                            \"short_code\":short_code,\n",
    "                                            \"post_url\":post_url,\n",
    "                                            \"location\":location\n",
    "                                            })\n",
    "\n",
    "                return page_info_df,posts_info_df\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        \n",
    "\n",
    "In \\[3\\]:\n",
    "\n",
    "    fi=Face_Scraping()\n",
    "    df=fi.get_page_posts(url=\"https://www.facebook.com/google\")\n",
    "\n",
    "In \\[4\\]:\n",
    "\n",
    "    fi=Face_Scraping()\n",
    "    df1=fi.get_page_info(url=\"https://www.facebook.com/google\")\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    ins=Insta_Scraping()\n",
    "    cur_info,cur_posts=ins.insta_2_scraper(\"google\")\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    df.head()\n",
    "\n",
    "Out\\[6\\]:\n",
    "\n",
    "|     | timestamp  | upload_date         | Content                                           | Num_Comments | Num_Shares | Total_Reacts | Likes | Sad | Angry | Love | Haha | Remaining_reacts |\n",
    "|-----|------------|---------------------|---------------------------------------------------|--------------|------------|--------------|-------|-----|-------|------|------|------------------|\n",
    "| 0   | 1652299670 | 2022-05-11 22:07:50 | At #GoogleIO today, Sundar Pichai talked about... | 163          | 38         | 611          | 553   | 0   | 0     | 48   | 0    | 10               |\n",
    "| 1   | 1653001245 | 2022-05-20 01:00:45 | Engineering lead Gordon Kuo talks about landin... | 7            | 3          | 110          | 100   | 0   | 0     | 8    | 0    | 2                |\n",
    "| 2   | 1652982521 | 2022-05-19 19:48:41 | On Global Accessibility Awareness Day (#GAAD),... | 23           | 11         | 204          | 168   | 0   | 0     | 20   | 0    | 16               |\n",
    "| 3   | 1652975099 | 2022-05-19 17:44:59 | Today's #GoogleDoodle celebrates Asian Pacific... | 11           | 36         | 199          | 159   | 0   | 0     | 27   | 0    | 13               |\n",
    "| 4   | 1652904126 | 2022-05-18 22:02:06 | Our new Bay View campus features a first-of-it... | 75           | 54         | 739          | 640   | 0   | 0     | 76   | 0    | 23               |\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    pd.DataFrame(df1,index=[x for x in range(1)])\n",
    "\n",
    "Out\\[7\\]:\n",
    "\n",
    "|     | pg_likes | pg_follows | pg_checked | pg_about                                          |\n",
    "|-----|----------|------------|------------|---------------------------------------------------|\n",
    "| 0   | 28180257 | 33029117   | 0          | Organizing the world's information and making ... |\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    cur_info\n",
    "\n",
    "Out\\[8\\]:\n",
    "\n",
    "|     | number_of_followers | number_of_followings | number_of_posts | profile_picture_url                               | is_verified | other_info | username | fullname | is_private | is_joined_recently | biography                                 | website                   |\n",
    "|-----|---------------------|----------------------|-----------------|---------------------------------------------------|-------------|------------|----------|----------|------------|--------------------|-------------------------------------------|---------------------------|\n",
    "| 0   | 13031372            | 33                   | 1798            | https://instagram.fcai2-1.fna.fbcdn.net/v/t51.... | True        | NaN        | google   | Google   | False      | False              | Google unfiltered—sometimes with filters. | https://linkin.bio/google |\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    cur_posts.head()\n",
    "\n",
    "Out\\[9\\]:\n",
    "\n",
    "|     | Likes | Num_Comments | upload_date         | caption                                           | timestamp  | display_url                                       | is_video | short_code  | post_url                                 | location                                          |\n",
    "|-----|-------|--------------|---------------------|---------------------------------------------------|------------|---------------------------------------------------|----------|-------------|------------------------------------------|---------------------------------------------------|\n",
    "| 0   | 7750  | 86           | 2022-05-19 21:29:55 | An aerial view of the Google Bay View Campus, ... | 1652988595 | https://instagram.fcai2-1.fna.fbcdn.net/v/t51.... | False    | CdwFxosOdvb | https://www.instagram.com/p/CdwFxosOdvb/ | None                                              |\n",
    "| 1   | 6066  | 101          | 2022-05-19 17:56:34 | Google logo artwork with Stacey Milbern on the... | 1652975794 | https://instagram.fcai2-1.fna.fbcdn.net/v/t51.... | False    | CdvtXC7O3Xj | https://www.instagram.com/p/CdvtXC7O3Xj/ | None                                              |\n",
    "| 2   | 4363  | 87           | 2022-05-19 00:42:50 | None                                              | 1652913770 | https://instagram.fcai2-1.fna.fbcdn.net/v/t51.... | True     | Cdt3Bi-DzAK | https://www.instagram.com/p/Cdt3Bi-DzAK/ | None                                              |\n",
    "| 3   | 38132 | 309          | 2022-05-17 23:12:24 | A building at the Google Bay View campus, phot... | 1652821944 | https://instagram.fcai2-2.fna.fbcdn.net/v/t51.... | False    | CdrH6kiudWa | https://www.instagram.com/p/CdrH6kiudWa/ | None                                              |\n",
    "| 4   | 21376 | 309          | 2022-05-12 04:46:36 | Image of a colorful sculpture that spells out ... | 1652323596 | https://instagram.fcai2-1.fna.fbcdn.net/v/t51.... | False    | CdcRZE4LWZX | https://www.instagram.com/p/CdcRZE4LWZX/ | {'id': '216796294', 'has_public_page': True, '... |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
